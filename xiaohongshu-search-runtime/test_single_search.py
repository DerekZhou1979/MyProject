#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ç®€åŒ–çš„å•ä¸ªæœç´¢æµ‹è¯•
"""

import os
import sys
import time
import logging
from crawler import XiaoHongShuCrawler

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_single_search(keyword="ç¾é£Ÿæ¨è"):
    """æµ‹è¯•å•ä¸ªå…³é”®è¯æœç´¢"""
    
    cookies_file = os.path.join('cache', 'xiaohongshu_cookies.json')
    
    # æ£€æŸ¥cookiesæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(cookies_file):
        logger.error(f"Cookieæ–‡ä»¶ä¸å­˜åœ¨: {cookies_file}")
        logger.info("è¯·å…ˆè¿è¡Œ python cookie_manager.py login è¿›è¡Œç™»å½•")
        return False
    
    logger.info(f"ğŸ” æµ‹è¯•æœç´¢å…³é”®è¯: '{keyword}'")
    
    crawler = None
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        crawler = XiaoHongShuCrawler(
            use_selenium=True,
            headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨ä»¥ä¾¿è§‚å¯Ÿ
            cookies_file=cookies_file,
            load_cookies=True
        )
        
        # æ‰§è¡Œæœç´¢
        logger.info(f"å¼€å§‹æœç´¢: {keyword}")
        start_time = time.time()
        results = crawler.search(keyword, max_results=3, use_cache=False)
        search_time = time.time() - start_time
        
        logger.info(f"â±ï¸  æœç´¢è€—æ—¶: {search_time:.2f} ç§’")
        logger.info(f"ğŸ“Š æœç´¢ç»“æœæ•°é‡: {len(results)}")
        
        if results:
            logger.info("âœ… æœç´¢æˆåŠŸï¼")
            
            # æ˜¾ç¤ºç»“æœ
            for i, note in enumerate(results, 1):
                logger.info(f"  {i}. æ ‡é¢˜: {note.get('title', 'æ— æ ‡é¢˜')}")
                logger.info(f"     ID: {note.get('id', 'N/A')}")
                logger.info(f"     URL: {note.get('url', 'N/A')}")
                logger.info("")
                
            return True
        else:
            logger.warning("âŒ æœç´¢æ— ç»“æœ")
            
            # æ£€æŸ¥ç¼“å­˜ç›®å½•
            cache_dir = crawler.cache_dir if crawler else 'cache'
            if os.path.exists(cache_dir):
                files = os.listdir(cache_dir)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯æˆªå›¾
                error_screenshots = [f for f in files if 'failed' in f and f.endswith('.png')]
                if error_screenshots:
                    latest = max(error_screenshots, key=lambda x: os.path.getctime(os.path.join(cache_dir, x)))
                    logger.info(f"ğŸ“· é”™è¯¯æˆªå›¾: {latest}")
                
                # æ£€æŸ¥é¡µé¢æºç 
                html_files = [f for f in files if f.startswith('page_source_')]
                if html_files:
                    latest = max(html_files, key=lambda x: os.path.getctime(os.path.join(cache_dir, x)))
                    logger.info(f"ğŸ“„ é¡µé¢æºç : {latest}")
            
            return False
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
        return False
    
    finally:
        # ç¡®ä¿å…³é—­æµè§ˆå™¨
        if crawler:
            try:
                crawler.close()
            except:
                pass

if __name__ == "__main__":
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæœç´¢å…³é”®è¯
    keyword = sys.argv[1] if len(sys.argv) > 1 else "ç¾é£Ÿæ¨è"
    
    logger.info("ğŸš€ å°çº¢ä¹¦å•ä¸ªæœç´¢æµ‹è¯•")
    logger.info("=" * 50)
    
    success = test_single_search(keyword)
    
    if success:
        logger.info("\nâœ… æµ‹è¯•æˆåŠŸï¼")
    else:
        logger.info("\nâŒ æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1) 