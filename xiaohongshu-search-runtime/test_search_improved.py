#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æµ‹è¯•æ”¹è¿›åçš„æœç´¢åŠŸèƒ½
"""

import os
import sys
import time
import logging
from crawler import XiaoHongShuCrawler

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('test_search.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

def test_search_with_improved_logic():
    """æµ‹è¯•æ”¹è¿›åçš„æœç´¢é€»è¾‘"""
    
    cookies_file = os.path.join('cache', 'xiaohongshu_cookies.json')
    
    # æ£€æŸ¥cookiesæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(cookies_file):
        logger.error(f"Cookieæ–‡ä»¶ä¸å­˜åœ¨: {cookies_file}")
        logger.info("è¯·å…ˆè¿è¡Œ python cookie_manager.py login è¿›è¡Œç™»å½•")
        return False
    
    logger.info("ğŸ” å¼€å§‹æµ‹è¯•æ”¹è¿›åçš„æœç´¢åŠŸèƒ½...")
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        crawler = XiaoHongShuCrawler(
            use_selenium=True,
            headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨ä»¥ä¾¿è§‚å¯Ÿ
            cookies_file=cookies_file,
            load_cookies=True
        )
        
        # æµ‹è¯•å…³é”®è¯
        test_keywords = [
            "ç¾é£Ÿæ¨è",
            "æŠ¤è‚¤å¿ƒå¾—", 
            "æ—…è¡Œæ”»ç•¥"
        ]
        
        for i, keyword in enumerate(test_keywords, 1):
            logger.info(f"\nğŸ“ æµ‹è¯• {i}: æœç´¢å…³é”®è¯ '{keyword}'")
            logger.info("=" * 50)
            
            try:
                # æ‰§è¡Œæœç´¢
                start_time = time.time()
                results = crawler.search(keyword, max_results=5, use_cache=False)
                search_time = time.time() - start_time
                
                logger.info(f"â±ï¸  æœç´¢è€—æ—¶: {search_time:.2f} ç§’")
                logger.info(f"ğŸ“Š æœç´¢ç»“æœæ•°é‡: {len(results)}")
                
                if results:
                    logger.info("âœ… æœç´¢æˆåŠŸï¼")
                    
                    # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                    for j, note in enumerate(results[:3], 1):
                        logger.info(f"  {j}. æ ‡é¢˜: {note.get('title', 'æ— æ ‡é¢˜')[:50]}")
                        logger.info(f"     ID: {note.get('id', 'N/A')}")
                        logger.info(f"     å°é¢: {'æœ‰' if note.get('cover') else 'æ— '}")
                        logger.info(f"     URL: {note.get('url', 'N/A')[:80]}")
                        logger.info("")
                else:
                    logger.warning("âŒ æœç´¢æ— ç»“æœ")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯æˆªå›¾
                    cache_dir = crawler.cache_dir
                    error_screenshots = [f for f in os.listdir(cache_dir) 
                                       if f.startswith('search_failed_') and f.endswith('.png')]
                    
                    if error_screenshots:
                        latest_screenshot = max(error_screenshots, 
                                              key=lambda x: os.path.getctime(os.path.join(cache_dir, x)))
                        logger.info(f"ğŸ“· å·²ä¿å­˜é”™è¯¯æˆªå›¾: {latest_screenshot}")
                    
                    # æ£€æŸ¥é¡µé¢æºç 
                    page_sources = [f for f in os.listdir(cache_dir) 
                                   if f.startswith('page_source_') and f.endswith('.html')]
                    
                    if page_sources:
                        latest_source = max(page_sources, 
                                          key=lambda x: os.path.getctime(os.path.join(cache_dir, x)))
                        logger.info(f"ğŸ“„ å·²ä¿å­˜é¡µé¢æºç : {latest_source}")
                
                # ç­‰å¾…ä¸€ä¸‹å†è¿›è¡Œä¸‹ä¸€æ¬¡æœç´¢
                if i < len(test_keywords):
                    logger.info("â³ ç­‰å¾… 3 ç§’åè¿›è¡Œä¸‹ä¸€æ¬¡æœç´¢...")
                    time.sleep(3)
                    
            except Exception as e:
                logger.error(f"âŒ æœç´¢å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {str(e)}")
                logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
        
        logger.info("\nğŸ¯ æµ‹è¯•å®Œæˆ!")
        
        # æ˜¾ç¤ºç¼“å­˜æ–‡ä»¶çŠ¶æ€
        cache_dir = crawler.cache_dir
        cache_files = os.listdir(cache_dir)
        
        logger.info(f"\nğŸ“ ç¼“å­˜ç›®å½•æ–‡ä»¶: {len(cache_files)} ä¸ª")
        
        # åˆ†ç±»ç»Ÿè®¡
        screenshots = len([f for f in cache_files if f.endswith('.png')])
        html_files = len([f for f in cache_files if f.endswith('.html')])
        json_files = len([f for f in cache_files if f.endswith('.json')])
        
        logger.info(f"   ğŸ“· æˆªå›¾æ–‡ä»¶: {screenshots} ä¸ª")
        logger.info(f"   ğŸ“„ HTMLæ–‡ä»¶: {html_files} ä¸ª") 
        logger.info(f"   ğŸ“‹ JSONæ–‡ä»¶: {json_files} ä¸ª")
        
        # å…³é—­æµè§ˆå™¨
        crawler.close()
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
        return False

def analyze_search_results():
    """åˆ†ææœç´¢ç»“æœå’Œæˆªå›¾"""
    logger.info("\nğŸ” åˆ†ææœç´¢ç»“æœ...")
    
    cache_dir = os.path.join(os.path.dirname(__file__), 'cache')
    
    if not os.path.exists(cache_dir):
        logger.warning("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return
    
    files = os.listdir(cache_dir)
    
    # åˆ†ææˆªå›¾æ–‡ä»¶
    success_screenshots = [f for f in files if f.startswith('search_success_')]
    failed_screenshots = [f for f in files if f.startswith('search_failed_')]
    error_screenshots = [f for f in files if f.startswith('search_error_')]
    
    logger.info(f"âœ… æˆåŠŸæœç´¢æˆªå›¾: {len(success_screenshots)} ä¸ª")
    logger.info(f"âŒ å¤±è´¥æœç´¢æˆªå›¾: {len(failed_screenshots)} ä¸ª")
    logger.info(f"ğŸš¨ é”™è¯¯æœç´¢æˆªå›¾: {len(error_screenshots)} ä¸ª")
    
    # æ˜¾ç¤ºæœ€æ–°çš„æˆªå›¾æ–‡ä»¶
    all_screenshots = success_screenshots + failed_screenshots + error_screenshots
    if all_screenshots:
        latest_screenshot = max(all_screenshots, 
                              key=lambda x: os.path.getctime(os.path.join(cache_dir, x)))
        logger.info(f"ğŸ“· æœ€æ–°æˆªå›¾: {latest_screenshot}")
        
        # æ˜¾ç¤ºæˆªå›¾æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
        screenshot_path = os.path.join(cache_dir, latest_screenshot)
        mtime = os.path.getmtime(screenshot_path)
        import datetime
        mtime_str = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        logger.info(f"   æ—¶é—´: {mtime_str}")
    
    # åˆ†æé¡µé¢æºç æ–‡ä»¶
    html_files = [f for f in files if f.startswith('page_source_')]
    logger.info(f"ğŸ“„ é¡µé¢æºç æ–‡ä»¶: {len(html_files)} ä¸ª")
    
    if html_files:
        latest_html = max(html_files, 
                         key=lambda x: os.path.getctime(os.path.join(cache_dir, x)))
        logger.info(f"ğŸ“„ æœ€æ–°é¡µé¢æºç : {latest_html}")
        
        # ç®€å•åˆ†æHTMLå†…å®¹
        html_path = os.path.join(cache_dir, latest_html)
        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥å…³é”®è¯
            has_error_indicators = any(indicator in content for indicator in [
                "ä½ è®¿é—®é¡µé¢ä¸è§äº†", "é¡µé¢ä¸è§äº†", "è¯·è¿”å›ä¸Šä¸€é¡µ",
                "éªŒè¯", "ç™»å½•", "è¯·å®Œæˆä¸‹åˆ—éªŒè¯"
            ])
            
            has_content_indicators = any(indicator in content for indicator in [
                "/explore/", "ç¬”è®°", "å°çº¢ä¹¦", "note-item", "feed-item"
            ])
            
            logger.info(f"   åŒ…å«é”™è¯¯æŒ‡ç¤ºå™¨: {'æ˜¯' if has_error_indicators else 'å¦'}")
            logger.info(f"   åŒ…å«å†…å®¹æŒ‡ç¤ºå™¨: {'æ˜¯' if has_content_indicators else 'å¦'}")
            
        except Exception as e:
            logger.warning(f"è¯»å–HTMLæ–‡ä»¶å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    logger.info("ğŸš€ å°çº¢ä¹¦æœç´¢åŠŸèƒ½æµ‹è¯•")
    logger.info("=" * 60)
    
    # è¿è¡Œæµ‹è¯•
    success = test_search_with_improved_logic()
    
    # åˆ†æç»“æœ
    analyze_search_results()
    
    if success:
        logger.info("\nâœ… æµ‹è¯•å®Œæˆï¼")
    else:
        logger.info("\nâŒ æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1) 